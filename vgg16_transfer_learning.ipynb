{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning to detect cats / dogs using Vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "Re-run this cell when starting from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset python environment\n",
    "%reset -f\n",
    "\n",
    "import time\n",
    "\n",
    "default_device = '/gpu:0'\n",
    "# default_device = '/cpu:0'\n",
    "\n",
    "num_hidden_neurons = 256\n",
    "\n",
    "vgg_mean = [103.939, 116.779, 123.68]\n",
    "classes = [l.strip() for l in open('synset.txt').readlines()]\n",
    "\n",
    "training_dataset_dir = './datasets/dogs-vs-cats-redux-kernels-edition/train/'\n",
    "test_dataset_dir = './datasets/dogs-vs-cats-redux-kernels-edition/test/'\n",
    "\n",
    "#model_version = int(time.time())\n",
    "model_version = 1\n",
    "model_path = 'models/model-{}'.format(model_version)\n",
    "\n",
    "def get_batches(x, y, batch_size=32):\n",
    "    num_rows = y.shape[0]\n",
    "    \n",
    "    num_batches = num_rows // batch_size\n",
    "    \n",
    "    if num_rows % batch_size != 0:\n",
    "        num_batches = num_batches + 1\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        yield x[batch_size * batch: batch_size * (batch + 1)], y[batch_size * batch: batch_size * (batch + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vgg16 Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Vgg16Model:\n",
    "    def __init__(self, weights_path='./vgg16.npy'):\n",
    "        self.weights = np.load('vgg16.npy', encoding='latin1').item()\n",
    "        self.activation_fn = tf.nn.relu\n",
    "        self.conv_padding = 'SAME'\n",
    "        self.pool_padding = 'SAME'\n",
    "        self.use_bias = True\n",
    "\n",
    "    def build(self, input_tensor, trainable=False):\n",
    "        self.conv1_1 = self.conv2d(input_tensor, 'conv1_1', 64, trainable)\n",
    "        self.conv1_2 = self.conv2d(self.conv1_1, 'conv1_2', 64, trainable)\n",
    "\n",
    "        # Max-pooling is performed over a 2 Ã— 2 pixel window, with stride 2.\n",
    "        self.max_pool1 = tf.layers.max_pooling2d(self.conv1_2, (2, 2), (2, 2), padding=self.pool_padding)\n",
    "\n",
    "        self.conv2_1 = self.conv2d(self.max_pool1, 'conv2_1', 128, trainable)\n",
    "        self.conv2_2 = self.conv2d(self.conv2_1, 'conv2_2', 128, trainable)\n",
    "\n",
    "        self.max_pool2 = tf.layers.max_pooling2d(self.conv2_2, (2, 2), (2, 2), padding=self.pool_padding)\n",
    "\n",
    "        self.conv3_1 = self.conv2d(self.max_pool2, 'conv3_1', 256, trainable)\n",
    "        self.conv3_2 = self.conv2d(self.conv3_1, 'conv3_2', 256, trainable)\n",
    "        self.conv3_3 = self.conv2d(self.conv3_2, 'conv3_3', 256, trainable)\n",
    "\n",
    "        self.max_pool3 = tf.layers.max_pooling2d(self.conv3_3, (2, 2), (2, 2), padding=self.pool_padding)\n",
    "\n",
    "        self.conv4_1 = self.conv2d(self.max_pool3, 'conv4_1', 512, trainable)\n",
    "        self.conv4_2 = self.conv2d(self.conv4_1, 'conv4_2', 512, trainable)\n",
    "        self.conv4_3 = self.conv2d(self.conv4_2, 'conv4_3', 512, trainable)\n",
    "\n",
    "        self.max_pool4 = tf.layers.max_pooling2d(self.conv4_3, (2, 2), (2, 2), padding=self.pool_padding)\n",
    "\n",
    "        self.conv5_1 = self.conv2d(self.max_pool4, 'conv5_1', 512, trainable)\n",
    "        self.conv5_2 = self.conv2d(self.conv5_1, 'conv5_2', 512, trainable)\n",
    "        self.conv5_3 = self.conv2d(self.conv5_2, 'conv5_3', 512, trainable)\n",
    "\n",
    "        self.max_pool5 = tf.layers.max_pooling2d(self.conv5_3, (2, 2), (2, 2), padding=self.pool_padding)\n",
    "\n",
    "        reshaped = tf.reshape(self.max_pool5, shape=(-1, 7 * 7 * 512))\n",
    "\n",
    "        self.fc6 = self.fc(reshaped, 'fc6', 4096, trainable)\n",
    "        self.fc7 = self.fc(self.fc6, 'fc7', 4096, trainable)\n",
    "\n",
    "        self.fc8 = self.fc(self.fc7, 'fc8', 1000, trainable)\n",
    "\n",
    "        self.predictions = tf.nn.softmax(self.fc8, name='predictions')\n",
    "\n",
    "    def conv2d(self, layer, name, n_filters, trainable, k_size=3):\n",
    "        return tf.layers.conv2d(layer, n_filters, kernel_size=(k_size, k_size),\n",
    "                                activation=self.activation_fn, padding=self.conv_padding, name=name, trainable=trainable,\n",
    "                                kernel_initializer=tf.constant_initializer(self.weights[name][0], dtype=tf.float32),\n",
    "                                bias_initializer=tf.constant_initializer(self.weights[name][1], dtype=tf.float32),\n",
    "                                use_bias=self.use_bias)\n",
    "\n",
    "    def fc(self, layer, name, size, trainable):\n",
    "        return tf.layers.dense(layer, size, activation=self.activation_fn,\n",
    "                               name=name, trainable=trainable,\n",
    "                               kernel_initializer=tf.constant_initializer(self.weights[name][0], dtype=tf.float32),\n",
    "                               bias_initializer=tf.constant_initializer(self.weights[name][1], dtype=tf.float32),\n",
    "                               use_bias=self.use_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images conversion for Vgg16\n",
    "\n",
    "Images have to be of dimension (224, 224, 3). The last dimension is ordered BGR (blue, green, red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "\n",
    "# https://github.com/machrisaa/tensorflow-vgg/blob/master/utils.py\n",
    "def load_image(image_path, mean=vgg_mean):\n",
    "    image = skimage.io.imread(image_path)\n",
    "\n",
    "    image = image.astype(float)\n",
    "    \n",
    "    short_edge = min(image.shape[:2])\n",
    "    yy = int((image.shape[0] - short_edge) / 2)\n",
    "    xx = int((image.shape[1] - short_edge) / 2)\n",
    "    crop_image = image[yy: yy + short_edge, xx: xx + short_edge]\n",
    "    \n",
    "    resized_image = skimage.transform.resize(crop_image, (224, 224), mode='constant') \n",
    "            \n",
    "    bgr = resized_image[:,:,::-1] - mean\n",
    "    \n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Vgg16 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import math\n",
    "\n",
    "def extract_codes(image_directory, batch_size=32):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # create mapping of filename -> vgg features\n",
    "    codes_fc6 = {}\n",
    "    codes_fc7 = {}\n",
    "    predictions = {}\n",
    "\n",
    "    filenames = os.listdir(image_directory)\n",
    "    num_files = len(filenames)\n",
    "    num_batches = int(math.ceil(num_files / batch_size))\n",
    "    \n",
    "    with tf.device(default_device):\n",
    "        with tf.Session(graph = tf.Graph()) as sess:    \n",
    "            _input = tf.placeholder(tf.float32, shape=(None, 224, 224, 3), name=\"images\")\n",
    "\n",
    "            vgg = Vgg16Model()\n",
    "            vgg.build(_input)\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                batch_filenames = filenames[i*batch_size : ((i+1)*batch_size)]\n",
    "\n",
    "                print(\"batch {} of {}\".format(i+1, num_batches))\n",
    "\n",
    "                start = time.time()\n",
    "                images = np.array([load_image(image_directory + f) for f in batch_filenames])\n",
    "                end = time.time()\n",
    "                print(\"\\timage loading took {:.4f} sec\".format(end-start))\n",
    "\n",
    "                start = end\n",
    "\n",
    "                batch_codes_fc6, batch_codes_fc7 = sess.run(\n",
    "                    [vgg.fc6, vgg.fc7],\n",
    "                    feed_dict={ _input: images }\n",
    "                )\n",
    "\n",
    "                end = time.time()\n",
    "                print(\"\\tprediction took {:.4f} sec\".format(end-start))\n",
    "\n",
    "                for i, filename in enumerate(batch_filenames):\n",
    "                    codes_fc6[filename] = batch_codes_fc6[i]\n",
    "                    codes_fc7[filename] = batch_codes_fc7[i]\n",
    "\n",
    "            return codes_fc6, codes_fc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Extracting training codes for fc6 and fc7')\n",
    "training_codes_fc6, training_codes_fc7 = extract_codes(training_dataset_dir)\n",
    "np.save('training_codes_fc6.npy', training_codes_fc6)\n",
    "np.save('training_codes_fc7.npy', training_codes_fc7)\n",
    "\n",
    "print('Extracting test codes for fc6 and fc7')\n",
    "test_codes_fc6, test_codes_fc7 = extract_codes(test_dataset_dir, batch_size=16)\n",
    "np.save('test_codes_fc6.npy', test_codes_fc6)\n",
    "np.save('test_codes_fc7.npy', test_codes_fc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint - Vgg16 features extracted and serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previously stored training codes (fc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "training_codes = np.load('training_codes_fc6.npy')\n",
    "training_codes = OrderedDict(training_codes.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = list(training_codes.keys())\n",
    "\n",
    "labels = np.array([ (1, 0) if name[:3] == 'dog' else (0,1) for name in keys]) # one hot encode labels\n",
    "\n",
    "images = np.array(list(training_codes.values())) # extract images\n",
    "\n",
    "for i,key in enumerate(keys):\n",
    "    assert (training_codes.get(key) == images[i]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    \n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1)\n",
    "train_indices, val_indices = next(splitter.split(images, labels))\n",
    "\n",
    "train_images, train_labels = images[train_indices], labels[train_indices]\n",
    "val_images, val_labels = images[val_indices], labels[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Step - Use a small NN with a single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "\n",
    "from tensorflow.python.saved_model.tag_constants import SERVING\n",
    "from tensorflow.python.saved_model.signature_constants import DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
    "from tensorflow.python.saved_model.signature_constants import PREDICT_INPUTS\n",
    "from tensorflow.python.saved_model.signature_constants import PREDICT_OUTPUTS\n",
    "            \n",
    "if(os.path.exists(model_path)):\n",
    "    raise Exception('directory \"{}\" already exists. Delete or move it'.format(model_path))\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "keep_prob = 0.5\n",
    "batch_size = 64\n",
    "accuracy_print_steps = 10\n",
    "iteration = 0\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(default_device):\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        \n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            _images = tf.placeholder(tf.float32, shape=(None, 4096), name='images')\n",
    "            _keep_prob = tf.placeholder(tf.float32, name='keep_probability')\n",
    "\n",
    "        with tf.name_scope(\"targets\"):\n",
    "            _labels = tf.placeholder(tf.float32, shape=(None, 2), name='labels')\n",
    "            \n",
    "        with tf.name_scope(\"hidden_layer\"):\n",
    "            hidden_weights = tf.Variable(\n",
    "                initial_value = tf.truncated_normal([4096, num_hidden_neurons], mean=0.0, stddev=0.01),\n",
    "                dtype=tf.float32, name=\"hidden_weights\"\n",
    "            )\n",
    "            \n",
    "            hidden_bias = tf.Variable(\n",
    "                initial_value = tf.zeros(num_hidden_neurons), \n",
    "                dtype=tf.float32,\n",
    "                name=\"hidden_bias\"\n",
    "            )\n",
    "            \n",
    "            hidden = tf.matmul(_images, hidden_weights) + hidden_bias\n",
    "            hidden = tf.nn.relu(hidden, name=\"hidden_relu\")\n",
    "            hidden = tf.nn.dropout(hidden, keep_prob=_keep_prob, name='hidden_dropout')\n",
    "            \n",
    "            tf.summary.histogram(\"hidden_weights\", hidden_weights)\n",
    "            tf.summary.histogram(\"hidden_bias\", hidden_bias)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"outputs\"):\n",
    "            output_weights = tf.Variable(\n",
    "                initial_value=tf.truncated_normal(shape=(num_hidden_neurons, 2), mean=0.0, stddev=0.01),\n",
    "                dtype=tf.float32, name=\"output_weights\"\n",
    "            )\n",
    "            \n",
    "            output_bias = tf.Variable(initial_value=tf.zeros(2), dtype=tf.float32, name=\"output_bias\")\n",
    "            \n",
    "            logits = tf.matmul(hidden, output_weights) + output_bias\n",
    "            predictions = tf.nn.softmax(logits, name='predictions')\n",
    "            \n",
    "            tf.summary.histogram(\"output_weights\", output_weights)\n",
    "            tf.summary.histogram(\"output_bias\", output_bias)\n",
    "            tf.summary.histogram(\"predictions\", predictions)\n",
    "    \n",
    "        with tf.name_scope(\"cost\"):\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=_labels, name='cross_entropy')\n",
    "            cost = tf.reduce_mean(cross_entropy, name='cost')\n",
    "            \n",
    "            tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(_labels, 1), name='correct_predictions')\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "        ### merge summaries\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        \n",
    "        ###  Save training and validation logs for tensorboard\n",
    "        train_writer = tf.summary.FileWriter('./logs/train/{}'.format(model_version), sess.graph)\n",
    "        val_writer = tf.summary.FileWriter('./logs/val/{}'.format(model_version))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_train_images, batch_train_labels in get_batches(train_images, train_labels, batch_size=batch_size):\n",
    "                train_loss, _, p, summary = sess.run(\n",
    "                    [cost, optimizer, logits, merged_summaries], \n",
    "                    feed_dict = { \n",
    "                        _images: batch_train_images,\n",
    "                        _labels: batch_train_labels,\n",
    "                        _keep_prob: keep_prob\n",
    "                    })\n",
    "\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                iteration = iteration + 1\n",
    "\n",
    "                if iteration % accuracy_print_steps == 0:\n",
    "                    val_acc, val_summary = sess.run([accuracy, merged_summaries], feed_dict ={\n",
    "                        _images: val_images,\n",
    "                        _labels: val_labels,\n",
    "                        _keep_prob: 1.\n",
    "                    })\n",
    "\n",
    "                    val_writer.add_summary(val_summary, iteration)\n",
    "                    print('{} / {} Accuracy: {} Loss: {}'.format(epoch + 1, num_epochs, val_acc, train_loss))\n",
    "        \n",
    "                \n",
    "\n",
    "        \n",
    "        ### Save graph and trained variables\n",
    "        builder = saved_model_builder.SavedModelBuilder(model_path)\n",
    "\n",
    "        builder.add_meta_graph_and_variables(\n",
    "            sess, [SERVING],\n",
    "            signature_def_map = {\n",
    "                DEFAULT_SERVING_SIGNATURE_DEF_KEY: predict_signature_def(\n",
    "                    inputs = { PREDICT_INPUTS: _images },\n",
    "                    outputs = { PREDICT_OUTPUTS: predictions }\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude -  Try to find optimal hyperparameters\n",
    "\n",
    "Run training with different hyperparameters and use tensorboard to investigate the best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "accuracy_print_steps = 100\n",
    "\n",
    "def train(writer, num_epochs, hidden_layer_size, learning_rate, num_hidden=1, keep_prob=0.5, batch_size=64):\n",
    "    with tf.device(default_device):\n",
    "        with tf.Session(graph=tf.Graph()) as sess:\n",
    "\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                _images = tf.placeholder(tf.float32, shape=(None, 4096), name='images')\n",
    "                _keep_prob = tf.placeholder(tf.float32, name='keep_probability')\n",
    "\n",
    "            with tf.name_scope(\"targets\"):\n",
    "                _labels = tf.placeholder(tf.float32, shape=(None, 2), name='labels')\n",
    "\n",
    "            prev_size = 4096\n",
    "            next_input = _images\n",
    "            \n",
    "            for i in range(num_hidden):\n",
    "                with tf.variable_scope(\"hidden_layer_{}\".format(i)):\n",
    "                    hidden_weights = tf.Variable(\n",
    "                        initial_value = tf.truncated_normal([prev_size, hidden_layer_size], mean=0.0, stddev=0.01),\n",
    "                        dtype=tf.float32, name=\"hidden_weights\"\n",
    "                    )\n",
    "\n",
    "                    hidden_bias = tf.Variable(\n",
    "                        initial_value = tf.zeros(hidden_layer_size), \n",
    "                        dtype=tf.float32,\n",
    "                        name=\"hidden_bias\"\n",
    "                    )\n",
    "\n",
    "                    hidden = tf.matmul(next_input, hidden_weights) + hidden_bias\n",
    "                    hidden = tf.layers.batch_normalization(hidden, training=True)\n",
    "                    hidden = tf.nn.relu(hidden, name=\"hidden_relu\")\n",
    "                    hidden = tf.nn.dropout(hidden, keep_prob=_keep_prob, name='hidden_dropout')\n",
    "\n",
    "                    tf.summary.histogram(\"hidden_weights_{}\".format(i), hidden_weights)\n",
    "                    tf.summary.histogram(\"hidden_bias_{}\".format(i), hidden_bias)\n",
    "                    \n",
    "                    next_input = hidden\n",
    "                    prev_size = hidden_layer_size\n",
    "\n",
    "\n",
    "            with tf.name_scope(\"outputs\"):\n",
    "                output_weights = tf.Variable(\n",
    "                    initial_value=tf.truncated_normal(shape=(hidden_layer_size, 2), mean=0.0, stddev=0.01),\n",
    "                    dtype=tf.float32, name=\"output_weights\"\n",
    "                )\n",
    "\n",
    "                output_bias = tf.Variable(initial_value=tf.zeros(2), dtype=tf.float32, name=\"output_bias\")\n",
    "\n",
    "                logits = tf.matmul(next_input, output_weights) + output_bias\n",
    "                predictions = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "                tf.summary.histogram(\"output_weights\", output_weights)\n",
    "                tf.summary.histogram(\"output_bias\", output_bias)\n",
    "                tf.summary.histogram(\"predictions\", predictions)\n",
    "\n",
    "            with tf.name_scope(\"cost\"):\n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=_labels, name='cross_entropy')\n",
    "                cost = tf.reduce_mean(cross_entropy, name='cost')\n",
    "\n",
    "                tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):                \n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                    correct_predictions = tf.equal(tf.argmax(predictions, 1), tf.argmax(_labels, 1), name='correct_predictions')\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "\n",
    "            ### merge summaries\n",
    "            merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            n_batches = math.ceil(train_images.shape[1] / batch_size)\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                batch_i = 0\n",
    "                \n",
    "                for batch_train_images, batch_train_labels in get_batches(train_images, train_labels, batch_size=batch_size):\n",
    "                    train_loss, _, p, summary = sess.run(\n",
    "                        [cost, optimizer, logits, merged_summaries], \n",
    "                        feed_dict = { \n",
    "                            _images: batch_train_images,\n",
    "                            _labels: batch_train_labels,\n",
    "                            _keep_prob: keep_prob\n",
    "                        })\n",
    "\n",
    "                    iteration = epoch * n_batches + batch_i\n",
    "                    batch_i = batch_i + 1\n",
    "\n",
    "                    writer.add_summary(summary, iteration)\n",
    "                    \n",
    "                    if iteration % accuracy_print_steps == 0:\n",
    "                        val_acc, val_summary = sess.run([accuracy, merged_summaries], feed_dict ={\n",
    "                            _images: val_images,\n",
    "                            _labels: val_labels,\n",
    "                            _keep_prob: 1.\n",
    "                        })\n",
    "                        \n",
    "\n",
    "                        print('\\tEpoch {}/{} Iteration {} Accuracy: {} Loss: {}'.format(epoch + 1, num_epochs, iteration, val_acc, train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for num_epochs in [1, 5]:\n",
    "    for keep_prob in [0.5, 0.8, 1.0]:\n",
    "        for num_hidden_layers in [1, 2]:\n",
    "            for hidden_layer_size in [256, 512, 1024, 2048]:\n",
    "                for learning_rate in [0.01, 0.001]:\n",
    "                    log_string = 'logs/{}/e={},lr={},hl={},hs={},kp={},bs={}'.format(model_version, num_epochs, learning_rate, num_hidden_layers, hidden_layer_size, keep_prob, batch_size)\n",
    "                    writer = tf.summary.FileWriter(log_string)\n",
    "\n",
    "                    print(\"\\n\\nStarting {}\".format(log_string))\n",
    "                    train(writer, num_epochs, hidden_layer_size, learning_rate, num_hidden_layers, keep_prob, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Checkpoint - Load previously stored test codes (fc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "test_codes = np.load('test_codes_fc6.npy')\n",
    "test_codes = OrderedDict(test_codes.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = list(test_codes.keys())\n",
    "\n",
    "images = np.array(list(test_codes.values()))\n",
    "\n",
    "keys = list(map(lambda k: k[:-4], keys))    \n",
    "keys = np.array(sorted(keys, key=int))\n",
    "\n",
    "examples = keys[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    a = fig.add_subplot(1,len(examples), i+1)\n",
    "    plt.imshow(skimage.io.imread(test_dataset_dir + example + '.jpg'))\n",
    "    # a.set_title(codes[examples[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create predictions for test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.saved_model import loader\n",
    "from tensorflow.python.saved_model.tag_constants import SERVING\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(default_device):\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        \n",
    "        loader.load(sess, [SERVING], model_path)\n",
    "\n",
    "        with open('out.csv', 'w') as f:\n",
    "            f.write('id,label\\n')\n",
    "\n",
    "            for b_images, b_keys in get_batches(images, keys):\n",
    "                \n",
    "                s_keep_probability = sess.graph.get_tensor_by_name('keep_probability:0')\n",
    "                s_images = sess.graph.get_tensor_by_name('images:0')\n",
    "                s_predictions = sess.graph.get_tensor_by_name('predictions:0')\n",
    "\n",
    "                preds = sess.run(s_predictions, feed_dict={\n",
    "                    s_images: b_images,\n",
    "                    s_keep_probability: 1.\n",
    "                })\n",
    "\n",
    "                for idx,pred in enumerate(preds):\n",
    "                    s = '{},{:.5f}\\n'.format(b_keys[idx], np.clip(pred[0], 0.05, 0.95))\n",
    "                    f.write(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
